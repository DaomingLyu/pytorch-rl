{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = gym.make('CartPole-v1')\n",
    "test_env = gym.make('CartPole-v1')\n",
    "\n",
    "assert isinstance(train_env.observation_space, gym.spaces.Box)\n",
    "assert isinstance(train_env.action_space, gym.spaces.Discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "train_env.seed(SEED);\n",
    "test_env.seed(SEED);\n",
    "np.random.seed(SEED);\n",
    "torch.manual_seed(SEED);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc_1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = train_env.observation_space.shape[0]\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = train_env.action_space.n\n",
    "\n",
    "actor = MLP(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "critic = MLP(INPUT_DIM, HIDDEN_DIM, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def init_weights(m):\\n    if type(m) == nn.Linear:\\n        torch.nn.init.xavier_normal_(m.weight)\\n        m.bias.data.fill_(0)\\n        \\nactor.apply(init_weights)\\ncritic.apply(init_weights)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "actor.apply(init_weights)\n",
    "critic.apply(init_weights)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=3e-4)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, actor, critic, actor_optimizer, critic_optimizer, n_steps, discount_factor):\n",
    "    \n",
    "    log_prob_actions = torch.zeros(n_steps)\n",
    "    entropies = torch.zeros(n_steps)\n",
    "    values = torch.zeros(n_steps)\n",
    "    rewards = torch.zeros(n_steps)\n",
    "    masks = torch.zeros(n_steps)\n",
    "    episode_reward = 0\n",
    "\n",
    "    state = env.state\n",
    "\n",
    "    for step in range(n_steps):\n",
    "\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        action_preds = actor(state)\n",
    "        value_pred = critic(state).squeeze(-1)\n",
    "\n",
    "        action_probs = F.softmax(action_preds, dim = -1)\n",
    "                \n",
    "        dist = distributions.Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        \n",
    "        log_prob_action = dist.log_prob(action)\n",
    "        \n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        log_prob_actions[step] = log_prob_action\n",
    "        entropies[step] = entropy\n",
    "        values[step] = value_pred\n",
    "        rewards[step] = reward\n",
    "        masks[step] = 1 - done\n",
    "    \n",
    "        if done:\n",
    "            state = env.reset()\n",
    "    \n",
    "    next_value = critic(torch.FloatTensor(state).unsqueeze(0)).squeeze(-1)\n",
    "    returns = calculate_returns(rewards, next_value, masks, discount_factor)\n",
    "    advantages = calculate_advantages(returns, values)\n",
    "    \n",
    "    policy_loss, value_loss = update_policy(advantages, log_prob_actions, returns, values, entropies, actor_optimizer, critic_optimizer)\n",
    "\n",
    "    return policy_loss, value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(rewards, next_value, masks, discount_factor, normalize = False):\n",
    "    \n",
    "    returns = torch.zeros_like(rewards)\n",
    "    R = next_value.item()\n",
    "    \n",
    "    for i, (r, m) in enumerate(zip(reversed(rewards), reversed(masks))):\n",
    "        R = r + R * discount_factor * m\n",
    "        returns[i] = R\n",
    "    \n",
    "    if normalize:\n",
    "        \n",
    "        returns = (returns - returns.mean()) / returns.std()\n",
    "        \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advantages(returns, values, normalize = False):\n",
    "    \n",
    "    advantages = returns - values\n",
    "    \n",
    "    if normalize:\n",
    "        \n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "        \n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(advantages, log_prob_actions, returns, values, entropies, actor_optimizer, critic_optimizer):\n",
    "        \n",
    "    advantages = advantages.detach()\n",
    "    returns = returns.detach()\n",
    "        \n",
    "    policy_loss = - (advantages * log_prob_actions).mean() - 0.001 * entropies.mean()\n",
    "    \n",
    "    value_loss = 0.5 * F.smooth_l1_loss(returns, values).mean()\n",
    "        \n",
    "    actor_optimizer.zero_grad()\n",
    "    critic_optimizer.zero_grad()\n",
    "    \n",
    "    policy_loss.backward()\n",
    "    value_loss.backward()\n",
    "    \n",
    "    actor_optimizer.step()\n",
    "    critic_optimizer.step()\n",
    "    \n",
    "    return policy_loss.item(), value_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, actor, critic):\n",
    "    \n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        action_preds = actor(state)\n",
    "        \n",
    "        action_probs = F.softmax(action_preds, dim = -1)\n",
    "        \n",
    "        dist = distributions.Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample() \n",
    "        \n",
    "        state, reward, done, _ = env.step(action.item())\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb94e377d17045e3ad239dd129e1affb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Steps:      0 | Mean Rewards:   8.00 |\n",
      "| Steps:   1000 | Mean Rewards:  17.73 |\n",
      "| Steps:   2000 | Mean Rewards:  20.05 |\n",
      "| Steps:   3000 | Mean Rewards:  22.32 |\n",
      "| Steps:   4000 | Mean Rewards:  22.20 |\n",
      "| Steps:   5000 | Mean Rewards:  19.16 |\n",
      "| Steps:   6000 | Mean Rewards:  15.52 |\n",
      "| Steps:   7000 | Mean Rewards:  17.20 |\n",
      "| Steps:   8000 | Mean Rewards:  18.68 |\n",
      "| Steps:   9000 | Mean Rewards:  20.40 |\n",
      "| Steps:  10000 | Mean Rewards:  22.84 |\n",
      "| Steps:  11000 | Mean Rewards:  25.56 |\n",
      "| Steps:  12000 | Mean Rewards:  22.88 |\n",
      "| Steps:  13000 | Mean Rewards:  19.24 |\n",
      "| Steps:  14000 | Mean Rewards:  22.16 |\n",
      "| Steps:  15000 | Mean Rewards:  20.68 |\n",
      "| Steps:  16000 | Mean Rewards:  22.04 |\n",
      "| Steps:  17000 | Mean Rewards:  19.84 |\n",
      "| Steps:  18000 | Mean Rewards:  22.96 |\n",
      "| Steps:  19000 | Mean Rewards:  23.00 |\n",
      "| Steps:  20000 | Mean Rewards:  21.80 |\n",
      "| Steps:  21000 | Mean Rewards:  20.76 |\n",
      "| Steps:  22000 | Mean Rewards:  25.92 |\n",
      "| Steps:  23000 | Mean Rewards:  28.28 |\n",
      "| Steps:  24000 | Mean Rewards:  25.84 |\n",
      "| Steps:  25000 | Mean Rewards:  24.76 |\n",
      "| Steps:  26000 | Mean Rewards:  25.72 |\n",
      "| Steps:  27000 | Mean Rewards:  26.16 |\n",
      "| Steps:  28000 | Mean Rewards:  26.96 |\n",
      "| Steps:  29000 | Mean Rewards:  26.04 |\n",
      "| Steps:  30000 | Mean Rewards:  24.52 |\n",
      "| Steps:  31000 | Mean Rewards:  27.84 |\n",
      "| Steps:  32000 | Mean Rewards:  26.16 |\n",
      "| Steps:  33000 | Mean Rewards:  27.72 |\n",
      "| Steps:  34000 | Mean Rewards:  29.04 |\n",
      "| Steps:  35000 | Mean Rewards:  24.08 |\n",
      "| Steps:  36000 | Mean Rewards:  21.96 |\n",
      "| Steps:  37000 | Mean Rewards:  23.40 |\n",
      "| Steps:  38000 | Mean Rewards:  31.04 |\n",
      "| Steps:  39000 | Mean Rewards:  34.08 |\n",
      "| Steps:  40000 | Mean Rewards:  29.76 |\n",
      "| Steps:  41000 | Mean Rewards:  31.24 |\n",
      "| Steps:  42000 | Mean Rewards:  35.32 |\n",
      "| Steps:  43000 | Mean Rewards:  35.80 |\n",
      "| Steps:  44000 | Mean Rewards:  34.36 |\n",
      "| Steps:  45000 | Mean Rewards:  36.80 |\n",
      "| Steps:  46000 | Mean Rewards:  39.56 |\n",
      "| Steps:  47000 | Mean Rewards:  39.76 |\n",
      "| Steps:  48000 | Mean Rewards:  40.68 |\n",
      "| Steps:  49000 | Mean Rewards:  39.16 |\n",
      "| Steps:  50000 | Mean Rewards:  32.64 |\n",
      "| Steps:  51000 | Mean Rewards:  32.16 |\n",
      "| Steps:  52000 | Mean Rewards:  30.84 |\n",
      "| Steps:  53000 | Mean Rewards:  33.36 |\n",
      "| Steps:  54000 | Mean Rewards:  37.24 |\n",
      "| Steps:  55000 | Mean Rewards:  47.64 |\n",
      "| Steps:  56000 | Mean Rewards:  42.76 |\n",
      "| Steps:  57000 | Mean Rewards:  33.72 |\n",
      "| Steps:  58000 | Mean Rewards:  32.84 |\n",
      "| Steps:  59000 | Mean Rewards:  31.92 |\n",
      "| Steps:  60000 | Mean Rewards:  35.48 |\n",
      "| Steps:  61000 | Mean Rewards:  34.56 |\n",
      "| Steps:  62000 | Mean Rewards:  36.28 |\n",
      "| Steps:  63000 | Mean Rewards:  39.12 |\n",
      "| Steps:  64000 | Mean Rewards:  44.48 |\n",
      "| Steps:  65000 | Mean Rewards:  48.64 |\n",
      "| Steps:  66000 | Mean Rewards:  50.08 |\n",
      "| Steps:  67000 | Mean Rewards:  45.76 |\n",
      "| Steps:  68000 | Mean Rewards:  52.32 |\n",
      "| Steps:  69000 | Mean Rewards:  50.72 |\n",
      "| Steps:  70000 | Mean Rewards:  55.24 |\n",
      "| Steps:  71000 | Mean Rewards:  57.88 |\n",
      "| Steps:  72000 | Mean Rewards:  50.88 |\n",
      "| Steps:  73000 | Mean Rewards:  39.76 |\n",
      "| Steps:  74000 | Mean Rewards:  41.96 |\n",
      "| Steps:  75000 | Mean Rewards:  59.00 |\n",
      "| Steps:  76000 | Mean Rewards:  61.84 |\n",
      "| Steps:  77000 | Mean Rewards:  66.08 |\n",
      "| Steps:  78000 | Mean Rewards:  53.48 |\n",
      "| Steps:  79000 | Mean Rewards:  52.76 |\n",
      "| Steps:  80000 | Mean Rewards:  57.60 |\n",
      "| Steps:  81000 | Mean Rewards:  64.52 |\n",
      "| Steps:  82000 | Mean Rewards:  61.16 |\n",
      "| Steps:  83000 | Mean Rewards:  58.00 |\n",
      "| Steps:  84000 | Mean Rewards:  61.56 |\n",
      "| Steps:  85000 | Mean Rewards:  63.32 |\n",
      "| Steps:  86000 | Mean Rewards:  66.20 |\n",
      "| Steps:  87000 | Mean Rewards:  75.00 |\n",
      "| Steps:  88000 | Mean Rewards:  72.00 |\n",
      "| Steps:  89000 | Mean Rewards:  90.52 |\n",
      "| Steps:  90000 | Mean Rewards:  98.72 |\n",
      "| Steps:  91000 | Mean Rewards:  91.68 |\n",
      "| Steps:  92000 | Mean Rewards:  80.64 |\n",
      "| Steps:  93000 | Mean Rewards:  84.12 |\n",
      "| Steps:  94000 | Mean Rewards:  77.44 |\n",
      "| Steps:  95000 | Mean Rewards:  65.36 |\n",
      "| Steps:  96000 | Mean Rewards:  45.68 |\n",
      "| Steps:  97000 | Mean Rewards:  37.60 |\n",
      "| Steps:  98000 | Mean Rewards:  37.32 |\n",
      "| Steps:  99000 | Mean Rewards:  43.64 |\n",
      "| Steps: 100000 | Mean Rewards:  61.40 |\n",
      "| Steps: 101000 | Mean Rewards:  65.60 |\n",
      "| Steps: 102000 | Mean Rewards:  57.08 |\n",
      "| Steps: 103000 | Mean Rewards:  64.00 |\n",
      "| Steps: 104000 | Mean Rewards:  66.56 |\n",
      "| Steps: 105000 | Mean Rewards:  81.48 |\n",
      "| Steps: 106000 | Mean Rewards:  95.72 |\n",
      "| Steps: 107000 | Mean Rewards: 100.40 |\n",
      "| Steps: 108000 | Mean Rewards:  93.52 |\n",
      "| Steps: 109000 | Mean Rewards:  85.60 |\n",
      "| Steps: 110000 | Mean Rewards:  74.64 |\n",
      "| Steps: 111000 | Mean Rewards:  76.72 |\n",
      "| Steps: 112000 | Mean Rewards:  81.16 |\n",
      "| Steps: 113000 | Mean Rewards:  90.72 |\n",
      "| Steps: 114000 | Mean Rewards: 103.68 |\n",
      "| Steps: 115000 | Mean Rewards:  91.28 |\n",
      "| Steps: 116000 | Mean Rewards:  81.60 |\n",
      "| Steps: 117000 | Mean Rewards:  68.20 |\n",
      "| Steps: 118000 | Mean Rewards:  79.16 |\n",
      "| Steps: 119000 | Mean Rewards:  85.80 |\n",
      "| Steps: 120000 | Mean Rewards:  78.24 |\n",
      "| Steps: 121000 | Mean Rewards:  93.68 |\n",
      "| Steps: 122000 | Mean Rewards: 115.64 |\n",
      "| Steps: 123000 | Mean Rewards: 123.16 |\n",
      "| Steps: 124000 | Mean Rewards: 107.28 |\n",
      "| Steps: 125000 | Mean Rewards:  96.36 |\n",
      "| Steps: 126000 | Mean Rewards: 120.04 |\n",
      "| Steps: 127000 | Mean Rewards: 130.80 |\n",
      "| Steps: 128000 | Mean Rewards: 135.00 |\n",
      "| Steps: 129000 | Mean Rewards: 109.64 |\n",
      "| Steps: 130000 | Mean Rewards: 118.60 |\n",
      "| Steps: 131000 | Mean Rewards: 121.88 |\n",
      "| Steps: 132000 | Mean Rewards: 119.72 |\n",
      "| Steps: 133000 | Mean Rewards: 130.84 |\n",
      "| Steps: 134000 | Mean Rewards: 116.96 |\n",
      "| Steps: 135000 | Mean Rewards: 124.32 |\n",
      "| Steps: 136000 | Mean Rewards: 121.60 |\n",
      "| Steps: 137000 | Mean Rewards: 136.76 |\n",
      "| Steps: 138000 | Mean Rewards: 129.44 |\n",
      "| Steps: 139000 | Mean Rewards: 139.80 |\n",
      "| Steps: 140000 | Mean Rewards: 144.08 |\n",
      "| Steps: 141000 | Mean Rewards: 133.16 |\n",
      "| Steps: 142000 | Mean Rewards: 137.44 |\n",
      "| Steps: 143000 | Mean Rewards: 121.92 |\n",
      "| Steps: 144000 | Mean Rewards: 138.72 |\n",
      "| Steps: 145000 | Mean Rewards: 143.64 |\n",
      "| Steps: 146000 | Mean Rewards: 154.28 |\n",
      "| Steps: 147000 | Mean Rewards: 184.16 |\n",
      "| Steps: 148000 | Mean Rewards: 162.92 |\n",
      "| Steps: 149000 | Mean Rewards: 146.20 |\n",
      "| Steps: 150000 | Mean Rewards: 133.64 |\n",
      "| Steps: 151000 | Mean Rewards: 144.72 |\n",
      "| Steps: 152000 | Mean Rewards: 168.52 |\n",
      "| Steps: 153000 | Mean Rewards: 127.32 |\n",
      "| Steps: 154000 | Mean Rewards: 120.48 |\n",
      "| Steps: 155000 | Mean Rewards: 144.00 |\n",
      "| Steps: 156000 | Mean Rewards: 165.60 |\n",
      "| Steps: 157000 | Mean Rewards: 166.20 |\n",
      "| Steps: 158000 | Mean Rewards: 161.00 |\n",
      "| Steps: 159000 | Mean Rewards: 161.20 |\n",
      "| Steps: 160000 | Mean Rewards: 184.52 |\n",
      "| Steps: 161000 | Mean Rewards: 188.52 |\n",
      "| Steps: 162000 | Mean Rewards: 162.96 |\n",
      "| Steps: 163000 | Mean Rewards: 148.36 |\n",
      "| Steps: 164000 | Mean Rewards: 136.52 |\n",
      "| Steps: 165000 | Mean Rewards: 133.32 |\n",
      "| Steps: 166000 | Mean Rewards: 136.48 |\n",
      "| Steps: 167000 | Mean Rewards: 145.76 |\n",
      "| Steps: 168000 | Mean Rewards: 149.84 |\n",
      "| Steps: 169000 | Mean Rewards: 143.76 |\n",
      "| Steps: 170000 | Mean Rewards: 140.20 |\n",
      "| Steps: 171000 | Mean Rewards: 138.16 |\n",
      "| Steps: 172000 | Mean Rewards: 151.12 |\n",
      "| Steps: 173000 | Mean Rewards: 134.36 |\n",
      "| Steps: 174000 | Mean Rewards: 141.16 |\n",
      "| Steps: 175000 | Mean Rewards: 122.60 |\n",
      "| Steps: 176000 | Mean Rewards: 108.20 |\n",
      "| Steps: 177000 | Mean Rewards: 108.28 |\n",
      "| Steps: 178000 | Mean Rewards: 140.92 |\n",
      "| Steps: 179000 | Mean Rewards: 140.92 |\n",
      "| Steps: 180000 | Mean Rewards: 128.32 |\n",
      "| Steps: 181000 | Mean Rewards: 134.64 |\n",
      "| Steps: 182000 | Mean Rewards: 154.84 |\n",
      "| Steps: 183000 | Mean Rewards: 151.20 |\n",
      "| Steps: 184000 | Mean Rewards: 130.12 |\n",
      "| Steps: 185000 | Mean Rewards: 136.48 |\n",
      "| Steps: 186000 | Mean Rewards: 147.80 |\n",
      "| Steps: 187000 | Mean Rewards: 157.80 |\n",
      "| Steps: 188000 | Mean Rewards: 185.72 |\n",
      "| Steps: 189000 | Mean Rewards: 194.40 |\n",
      "| Steps: 190000 | Mean Rewards: 175.52 |\n",
      "| Steps: 191000 | Mean Rewards: 163.40 |\n",
      "| Steps: 192000 | Mean Rewards: 156.68 |\n",
      "| Steps: 193000 | Mean Rewards: 151.88 |\n",
      "| Steps: 194000 | Mean Rewards: 129.64 |\n",
      "| Steps: 195000 | Mean Rewards: 128.16 |\n",
      "| Steps: 196000 | Mean Rewards: 129.24 |\n",
      "| Steps: 197000 | Mean Rewards: 146.08 |\n",
      "| Steps: 198000 | Mean Rewards: 154.28 |\n",
      "| Steps: 199000 | Mean Rewards: 191.04 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Steps: 200000 | Mean Rewards: 167.32 |\n",
      "| Steps: 201000 | Mean Rewards: 151.08 |\n",
      "| Steps: 202000 | Mean Rewards: 137.28 |\n",
      "| Steps: 203000 | Mean Rewards: 135.60 |\n",
      "| Steps: 204000 | Mean Rewards: 112.80 |\n",
      "| Steps: 205000 | Mean Rewards: 126.84 |\n",
      "| Steps: 206000 | Mean Rewards: 143.48 |\n",
      "| Steps: 207000 | Mean Rewards: 150.52 |\n",
      "| Steps: 208000 | Mean Rewards: 148.32 |\n",
      "| Steps: 209000 | Mean Rewards: 166.24 |\n",
      "| Steps: 210000 | Mean Rewards: 156.04 |\n",
      "| Steps: 211000 | Mean Rewards: 158.96 |\n",
      "| Steps: 212000 | Mean Rewards: 144.52 |\n",
      "| Steps: 213000 | Mean Rewards: 147.12 |\n",
      "| Steps: 214000 | Mean Rewards: 168.76 |\n",
      "| Steps: 215000 | Mean Rewards: 180.16 |\n",
      "| Steps: 216000 | Mean Rewards: 175.96 |\n",
      "| Steps: 217000 | Mean Rewards: 173.72 |\n",
      "| Steps: 218000 | Mean Rewards: 163.16 |\n",
      "| Steps: 219000 | Mean Rewards: 163.88 |\n",
      "| Steps: 220000 | Mean Rewards: 172.76 |\n",
      "| Steps: 221000 | Mean Rewards: 173.48 |\n",
      "| Steps: 222000 | Mean Rewards: 162.28 |\n",
      "| Steps: 223000 | Mean Rewards: 177.72 |\n",
      "| Steps: 224000 | Mean Rewards: 165.56 |\n",
      "| Steps: 225000 | Mean Rewards: 168.76 |\n",
      "| Steps: 226000 | Mean Rewards: 192.40 |\n",
      "| Steps: 227000 | Mean Rewards: 195.72 |\n",
      "| Steps: 228000 | Mean Rewards: 175.40 |\n",
      "| Steps: 229000 | Mean Rewards: 182.64 |\n",
      "| Steps: 230000 | Mean Rewards: 201.48 |\n",
      "| Steps: 231000 | Mean Rewards: 204.04 |\n",
      "| Steps: 232000 | Mean Rewards: 205.04 |\n",
      "| Steps: 233000 | Mean Rewards: 174.64 |\n",
      "| Steps: 234000 | Mean Rewards: 162.96 |\n",
      "| Steps: 235000 | Mean Rewards: 172.84 |\n",
      "| Steps: 236000 | Mean Rewards: 187.76 |\n",
      "| Steps: 237000 | Mean Rewards: 154.68 |\n",
      "| Steps: 238000 | Mean Rewards: 139.36 |\n",
      "| Steps: 239000 | Mean Rewards: 112.20 |\n",
      "| Steps: 240000 | Mean Rewards: 106.36 |\n",
      "| Steps: 241000 | Mean Rewards:  95.12 |\n",
      "| Steps: 242000 | Mean Rewards: 105.28 |\n",
      "| Steps: 243000 | Mean Rewards: 118.16 |\n",
      "| Steps: 244000 | Mean Rewards: 125.96 |\n",
      "| Steps: 245000 | Mean Rewards: 152.40 |\n",
      "| Steps: 246000 | Mean Rewards: 174.68 |\n",
      "| Steps: 247000 | Mean Rewards: 167.92 |\n",
      "| Steps: 248000 | Mean Rewards: 165.36 |\n",
      "| Steps: 249000 | Mean Rewards: 153.84 |\n",
      "| Steps: 250000 | Mean Rewards: 143.72 |\n",
      "| Steps: 251000 | Mean Rewards: 168.48 |\n",
      "| Steps: 252000 | Mean Rewards: 185.44 |\n",
      "| Steps: 253000 | Mean Rewards: 218.48 |\n",
      "| Steps: 254000 | Mean Rewards: 209.08 |\n",
      "| Steps: 255000 | Mean Rewards: 206.76 |\n",
      "| Steps: 256000 | Mean Rewards: 219.44 |\n",
      "| Steps: 257000 | Mean Rewards: 221.76 |\n",
      "| Steps: 258000 | Mean Rewards: 221.00 |\n",
      "| Steps: 259000 | Mean Rewards: 214.84 |\n",
      "| Steps: 260000 | Mean Rewards: 216.40 |\n",
      "| Steps: 261000 | Mean Rewards: 228.80 |\n",
      "| Steps: 262000 | Mean Rewards: 231.68 |\n",
      "| Steps: 263000 | Mean Rewards: 222.48 |\n",
      "| Steps: 264000 | Mean Rewards: 247.44 |\n",
      "| Steps: 265000 | Mean Rewards: 243.24 |\n",
      "| Steps: 266000 | Mean Rewards: 204.28 |\n",
      "| Steps: 267000 | Mean Rewards: 182.72 |\n",
      "| Steps: 268000 | Mean Rewards: 182.32 |\n",
      "| Steps: 269000 | Mean Rewards: 187.52 |\n",
      "| Steps: 270000 | Mean Rewards: 186.88 |\n",
      "| Steps: 271000 | Mean Rewards: 167.52 |\n",
      "| Steps: 272000 | Mean Rewards: 162.16 |\n",
      "| Steps: 273000 | Mean Rewards: 167.68 |\n",
      "| Steps: 274000 | Mean Rewards: 174.04 |\n",
      "| Steps: 275000 | Mean Rewards: 175.44 |\n",
      "| Steps: 276000 | Mean Rewards: 178.44 |\n",
      "| Steps: 277000 | Mean Rewards: 171.60 |\n",
      "| Steps: 278000 | Mean Rewards: 164.48 |\n",
      "| Steps: 279000 | Mean Rewards: 150.92 |\n",
      "| Steps: 280000 | Mean Rewards: 172.76 |\n",
      "| Steps: 281000 | Mean Rewards: 178.44 |\n",
      "| Steps: 282000 | Mean Rewards: 195.60 |\n",
      "| Steps: 283000 | Mean Rewards: 181.16 |\n",
      "| Steps: 284000 | Mean Rewards: 144.16 |\n",
      "| Steps: 285000 | Mean Rewards: 131.88 |\n",
      "| Steps: 286000 | Mean Rewards: 131.32 |\n",
      "| Steps: 287000 | Mean Rewards: 137.64 |\n",
      "| Steps: 288000 | Mean Rewards: 160.92 |\n",
      "| Steps: 289000 | Mean Rewards: 191.68 |\n",
      "| Steps: 290000 | Mean Rewards: 212.32 |\n",
      "| Steps: 291000 | Mean Rewards: 222.96 |\n",
      "| Steps: 292000 | Mean Rewards: 201.24 |\n",
      "| Steps: 293000 | Mean Rewards: 197.84 |\n",
      "| Steps: 294000 | Mean Rewards: 204.84 |\n",
      "| Steps: 295000 | Mean Rewards: 205.92 |\n",
      "| Steps: 296000 | Mean Rewards: 224.40 |\n",
      "| Steps: 297000 | Mean Rewards: 220.68 |\n",
      "| Steps: 298000 | Mean Rewards: 223.28 |\n",
      "| Steps: 299000 | Mean Rewards: 220.12 |\n",
      "| Steps: 300000 | Mean Rewards: 215.12 |\n",
      "| Steps: 301000 | Mean Rewards: 220.76 |\n",
      "| Steps: 302000 | Mean Rewards: 232.64 |\n",
      "| Steps: 303000 | Mean Rewards: 226.76 |\n",
      "| Steps: 304000 | Mean Rewards: 235.64 |\n",
      "| Steps: 305000 | Mean Rewards: 278.04 |\n",
      "| Steps: 306000 | Mean Rewards: 288.92 |\n",
      "| Steps: 307000 | Mean Rewards: 256.64 |\n",
      "| Steps: 308000 | Mean Rewards: 254.80 |\n",
      "| Steps: 309000 | Mean Rewards: 264.04 |\n",
      "| Steps: 310000 | Mean Rewards: 244.88 |\n",
      "| Steps: 311000 | Mean Rewards: 235.48 |\n",
      "| Steps: 312000 | Mean Rewards: 244.60 |\n",
      "| Steps: 313000 | Mean Rewards: 228.60 |\n",
      "| Steps: 314000 | Mean Rewards: 222.68 |\n",
      "| Steps: 315000 | Mean Rewards: 203.88 |\n",
      "| Steps: 316000 | Mean Rewards: 231.20 |\n",
      "| Steps: 317000 | Mean Rewards: 257.56 |\n",
      "| Steps: 318000 | Mean Rewards: 249.56 |\n",
      "| Steps: 319000 | Mean Rewards: 239.08 |\n",
      "| Steps: 320000 | Mean Rewards: 236.28 |\n",
      "| Steps: 321000 | Mean Rewards: 254.92 |\n",
      "| Steps: 322000 | Mean Rewards: 253.32 |\n",
      "| Steps: 323000 | Mean Rewards: 291.20 |\n",
      "| Steps: 324000 | Mean Rewards: 298.32 |\n",
      "| Steps: 325000 | Mean Rewards: 302.24 |\n",
      "| Steps: 326000 | Mean Rewards: 276.76 |\n",
      "| Steps: 327000 | Mean Rewards: 270.36 |\n",
      "| Steps: 328000 | Mean Rewards: 270.60 |\n",
      "| Steps: 329000 | Mean Rewards: 305.20 |\n",
      "| Steps: 330000 | Mean Rewards: 328.52 |\n",
      "| Steps: 331000 | Mean Rewards: 335.52 |\n",
      "| Steps: 332000 | Mean Rewards: 291.92 |\n",
      "| Steps: 333000 | Mean Rewards: 283.12 |\n",
      "| Steps: 334000 | Mean Rewards: 269.40 |\n",
      "| Steps: 335000 | Mean Rewards: 300.32 |\n",
      "| Steps: 336000 | Mean Rewards: 315.44 |\n",
      "| Steps: 337000 | Mean Rewards: 336.04 |\n",
      "| Steps: 338000 | Mean Rewards: 337.76 |\n",
      "| Steps: 339000 | Mean Rewards: 353.68 |\n",
      "| Steps: 340000 | Mean Rewards: 325.88 |\n",
      "| Steps: 341000 | Mean Rewards: 346.80 |\n",
      "| Steps: 342000 | Mean Rewards: 306.32 |\n",
      "| Steps: 343000 | Mean Rewards: 304.80 |\n",
      "| Steps: 344000 | Mean Rewards: 254.60 |\n",
      "| Steps: 345000 | Mean Rewards: 238.40 |\n",
      "| Steps: 346000 | Mean Rewards: 217.16 |\n",
      "| Steps: 347000 | Mean Rewards: 212.84 |\n",
      "| Steps: 348000 | Mean Rewards: 212.40 |\n",
      "| Steps: 349000 | Mean Rewards: 190.36 |\n",
      "| Steps: 350000 | Mean Rewards: 233.08 |\n",
      "| Steps: 351000 | Mean Rewards: 249.32 |\n",
      "| Steps: 352000 | Mean Rewards: 256.96 |\n",
      "| Steps: 353000 | Mean Rewards: 265.76 |\n",
      "| Steps: 354000 | Mean Rewards: 235.76 |\n",
      "| Steps: 355000 | Mean Rewards: 206.04 |\n",
      "| Steps: 356000 | Mean Rewards: 179.16 |\n",
      "| Steps: 357000 | Mean Rewards: 185.80 |\n",
      "| Steps: 358000 | Mean Rewards: 191.88 |\n",
      "| Steps: 359000 | Mean Rewards: 161.20 |\n",
      "| Steps: 360000 | Mean Rewards: 135.44 |\n",
      "| Steps: 361000 | Mean Rewards: 129.12 |\n",
      "| Steps: 362000 | Mean Rewards: 131.96 |\n",
      "| Steps: 363000 | Mean Rewards: 146.24 |\n",
      "| Steps: 364000 | Mean Rewards: 161.68 |\n",
      "| Steps: 365000 | Mean Rewards: 158.60 |\n",
      "| Steps: 366000 | Mean Rewards: 147.12 |\n",
      "| Steps: 367000 | Mean Rewards: 153.48 |\n",
      "| Steps: 368000 | Mean Rewards: 160.32 |\n",
      "| Steps: 369000 | Mean Rewards: 167.16 |\n",
      "| Steps: 370000 | Mean Rewards: 165.16 |\n",
      "| Steps: 371000 | Mean Rewards: 167.40 |\n",
      "| Steps: 372000 | Mean Rewards: 163.84 |\n",
      "| Steps: 373000 | Mean Rewards: 200.48 |\n",
      "| Steps: 374000 | Mean Rewards: 231.32 |\n",
      "| Steps: 375000 | Mean Rewards: 258.80 |\n",
      "| Steps: 376000 | Mean Rewards: 274.72 |\n",
      "| Steps: 377000 | Mean Rewards: 291.00 |\n",
      "| Steps: 378000 | Mean Rewards: 325.60 |\n",
      "| Steps: 379000 | Mean Rewards: 326.44 |\n",
      "| Steps: 380000 | Mean Rewards: 307.16 |\n",
      "| Steps: 381000 | Mean Rewards: 270.52 |\n",
      "| Steps: 382000 | Mean Rewards: 265.00 |\n",
      "| Steps: 383000 | Mean Rewards: 298.44 |\n",
      "| Steps: 384000 | Mean Rewards: 312.76 |\n",
      "| Steps: 385000 | Mean Rewards: 326.28 |\n",
      "| Steps: 386000 | Mean Rewards: 322.08 |\n",
      "| Steps: 387000 | Mean Rewards: 324.80 |\n",
      "| Steps: 388000 | Mean Rewards: 328.44 |\n",
      "| Steps: 389000 | Mean Rewards: 323.24 |\n",
      "| Steps: 390000 | Mean Rewards: 329.08 |\n",
      "| Steps: 391000 | Mean Rewards: 383.32 |\n",
      "| Steps: 392000 | Mean Rewards: 371.12 |\n",
      "| Steps: 393000 | Mean Rewards: 375.16 |\n",
      "| Steps: 394000 | Mean Rewards: 359.24 |\n",
      "| Steps: 395000 | Mean Rewards: 331.04 |\n",
      "| Steps: 396000 | Mean Rewards: 326.72 |\n",
      "| Steps: 397000 | Mean Rewards: 342.48 |\n",
      "| Steps: 398000 | Mean Rewards: 369.20 |\n",
      "| Steps: 399000 | Mean Rewards: 374.64 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Steps: 400000 | Mean Rewards: 373.04 |\n",
      "| Steps: 401000 | Mean Rewards: 399.16 |\n",
      "| Steps: 402000 | Mean Rewards: 391.84 |\n",
      "| Steps: 403000 | Mean Rewards: 394.60 |\n",
      "| Steps: 404000 | Mean Rewards: 394.44 |\n",
      "| Steps: 405000 | Mean Rewards: 391.80 |\n",
      "| Steps: 406000 | Mean Rewards: 397.52 |\n",
      "| Steps: 407000 | Mean Rewards: 409.84 |\n",
      "| Steps: 408000 | Mean Rewards: 380.64 |\n",
      "| Steps: 409000 | Mean Rewards: 340.48 |\n",
      "| Steps: 410000 | Mean Rewards: 338.44 |\n",
      "| Steps: 411000 | Mean Rewards: 322.60 |\n",
      "| Steps: 412000 | Mean Rewards: 281.36 |\n",
      "| Steps: 413000 | Mean Rewards: 234.84 |\n",
      "| Steps: 414000 | Mean Rewards: 240.76 |\n",
      "| Steps: 415000 | Mean Rewards: 247.20 |\n",
      "| Steps: 416000 | Mean Rewards: 267.56 |\n",
      "| Steps: 417000 | Mean Rewards: 262.96 |\n",
      "| Steps: 418000 | Mean Rewards: 243.92 |\n",
      "| Steps: 419000 | Mean Rewards: 234.76 |\n",
      "| Steps: 420000 | Mean Rewards: 233.92 |\n",
      "| Steps: 421000 | Mean Rewards: 245.84 |\n",
      "| Steps: 422000 | Mean Rewards: 258.00 |\n",
      "| Steps: 423000 | Mean Rewards: 289.04 |\n",
      "| Steps: 424000 | Mean Rewards: 328.48 |\n",
      "| Steps: 425000 | Mean Rewards: 335.88 |\n",
      "| Steps: 426000 | Mean Rewards: 352.88 |\n",
      "| Steps: 427000 | Mean Rewards: 351.88 |\n",
      "| Steps: 428000 | Mean Rewards: 361.84 |\n",
      "| Steps: 429000 | Mean Rewards: 338.68 |\n",
      "| Steps: 430000 | Mean Rewards: 372.84 |\n",
      "| Steps: 431000 | Mean Rewards: 389.92 |\n",
      "| Steps: 432000 | Mean Rewards: 370.84 |\n",
      "| Steps: 433000 | Mean Rewards: 407.24 |\n",
      "| Steps: 434000 | Mean Rewards: 422.32 |\n",
      "| Steps: 435000 | Mean Rewards: 409.48 |\n",
      "| Steps: 436000 | Mean Rewards: 406.16 |\n",
      "| Steps: 437000 | Mean Rewards: 372.96 |\n",
      "| Steps: 438000 | Mean Rewards: 379.08 |\n",
      "| Steps: 439000 | Mean Rewards: 393.68 |\n",
      "| Steps: 440000 | Mean Rewards: 382.52 |\n",
      "| Steps: 441000 | Mean Rewards: 392.72 |\n",
      "| Steps: 442000 | Mean Rewards: 374.88 |\n",
      "| Steps: 443000 | Mean Rewards: 363.88 |\n",
      "| Steps: 444000 | Mean Rewards: 349.24 |\n",
      "| Steps: 445000 | Mean Rewards: 350.28 |\n",
      "| Steps: 446000 | Mean Rewards: 370.36 |\n",
      "| Steps: 447000 | Mean Rewards: 384.60 |\n",
      "| Steps: 448000 | Mean Rewards: 368.52 |\n",
      "| Steps: 449000 | Mean Rewards: 383.60 |\n",
      "| Steps: 450000 | Mean Rewards: 393.36 |\n",
      "| Steps: 451000 | Mean Rewards: 413.28 |\n",
      "| Steps: 452000 | Mean Rewards: 389.00 |\n",
      "| Steps: 453000 | Mean Rewards: 389.88 |\n",
      "| Steps: 454000 | Mean Rewards: 412.56 |\n",
      "| Steps: 455000 | Mean Rewards: 417.24 |\n",
      "| Steps: 456000 | Mean Rewards: 386.56 |\n",
      "| Steps: 457000 | Mean Rewards: 373.36 |\n",
      "| Steps: 458000 | Mean Rewards: 378.68 |\n",
      "| Steps: 459000 | Mean Rewards: 391.40 |\n",
      "| Steps: 460000 | Mean Rewards: 389.68 |\n",
      "| Steps: 461000 | Mean Rewards: 372.00 |\n",
      "| Steps: 462000 | Mean Rewards: 344.52 |\n",
      "| Steps: 463000 | Mean Rewards: 353.44 |\n",
      "| Steps: 464000 | Mean Rewards: 342.20 |\n",
      "| Steps: 465000 | Mean Rewards: 314.08 |\n",
      "| Steps: 466000 | Mean Rewards: 282.88 |\n",
      "| Steps: 467000 | Mean Rewards: 248.40 |\n",
      "| Steps: 468000 | Mean Rewards: 226.08 |\n",
      "| Steps: 469000 | Mean Rewards: 206.08 |\n",
      "| Steps: 470000 | Mean Rewards: 209.48 |\n",
      "| Steps: 471000 | Mean Rewards: 219.40 |\n",
      "| Steps: 472000 | Mean Rewards: 200.72 |\n",
      "| Steps: 473000 | Mean Rewards: 176.76 |\n",
      "| Steps: 474000 | Mean Rewards: 174.24 |\n",
      "| Steps: 475000 | Mean Rewards: 155.20 |\n",
      "| Steps: 476000 | Mean Rewards: 136.72 |\n",
      "| Steps: 477000 | Mean Rewards: 119.52 |\n",
      "| Steps: 478000 | Mean Rewards: 120.80 |\n",
      "| Steps: 479000 | Mean Rewards: 120.56 |\n",
      "| Steps: 480000 | Mean Rewards: 127.72 |\n",
      "| Steps: 481000 | Mean Rewards: 158.56 |\n",
      "| Steps: 482000 | Mean Rewards: 177.20 |\n",
      "| Steps: 483000 | Mean Rewards: 190.68 |\n",
      "| Steps: 484000 | Mean Rewards: 246.24 |\n",
      "| Steps: 485000 | Mean Rewards: 307.20 |\n",
      "| Steps: 486000 | Mean Rewards: 334.68 |\n",
      "| Steps: 487000 | Mean Rewards: 322.72 |\n",
      "| Steps: 488000 | Mean Rewards: 315.64 |\n",
      "| Steps: 489000 | Mean Rewards: 300.24 |\n",
      "| Steps: 490000 | Mean Rewards: 301.84 |\n",
      "| Steps: 491000 | Mean Rewards: 327.24 |\n",
      "| Steps: 492000 | Mean Rewards: 294.04 |\n",
      "| Steps: 493000 | Mean Rewards: 283.68 |\n",
      "| Steps: 494000 | Mean Rewards: 301.08 |\n",
      "| Steps: 495000 | Mean Rewards: 307.60 |\n",
      "| Steps: 496000 | Mean Rewards: 290.64 |\n",
      "| Steps: 497000 | Mean Rewards: 240.24 |\n",
      "| Steps: 498000 | Mean Rewards: 188.68 |\n",
      "| Steps: 499000 | Mean Rewards: 187.16 |\n",
      "| Steps: 500000 | Mean Rewards: 181.60 |\n",
      "| Steps: 501000 | Mean Rewards: 186.96 |\n",
      "| Steps: 502000 | Mean Rewards: 173.28 |\n",
      "| Steps: 503000 | Mean Rewards: 173.72 |\n",
      "| Steps: 504000 | Mean Rewards: 156.96 |\n",
      "| Steps: 505000 | Mean Rewards: 153.12 |\n",
      "| Steps: 506000 | Mean Rewards: 145.48 |\n",
      "| Steps: 507000 | Mean Rewards: 150.92 |\n",
      "| Steps: 508000 | Mean Rewards: 152.08 |\n",
      "| Steps: 509000 | Mean Rewards: 186.76 |\n",
      "| Steps: 510000 | Mean Rewards: 218.64 |\n",
      "| Steps: 511000 | Mean Rewards: 229.60 |\n",
      "| Steps: 512000 | Mean Rewards: 279.12 |\n",
      "| Steps: 513000 | Mean Rewards: 339.88 |\n",
      "| Steps: 514000 | Mean Rewards: 375.40 |\n",
      "| Steps: 515000 | Mean Rewards: 367.12 |\n",
      "| Steps: 516000 | Mean Rewards: 351.40 |\n",
      "| Steps: 517000 | Mean Rewards: 331.40 |\n",
      "| Steps: 518000 | Mean Rewards: 349.88 |\n",
      "| Steps: 519000 | Mean Rewards: 393.52 |\n",
      "| Steps: 520000 | Mean Rewards: 360.60 |\n",
      "| Steps: 521000 | Mean Rewards: 384.84 |\n",
      "| Steps: 522000 | Mean Rewards: 348.24 |\n",
      "| Steps: 523000 | Mean Rewards: 328.40 |\n",
      "| Steps: 524000 | Mean Rewards: 302.44 |\n",
      "| Steps: 525000 | Mean Rewards: 305.12 |\n",
      "| Steps: 526000 | Mean Rewards: 279.68 |\n",
      "| Steps: 527000 | Mean Rewards: 290.48 |\n",
      "| Steps: 528000 | Mean Rewards: 267.72 |\n",
      "| Steps: 529000 | Mean Rewards: 258.40 |\n",
      "| Steps: 530000 | Mean Rewards: 260.08 |\n",
      "| Steps: 531000 | Mean Rewards: 253.28 |\n",
      "| Steps: 532000 | Mean Rewards: 251.64 |\n",
      "| Steps: 533000 | Mean Rewards: 270.96 |\n",
      "| Steps: 534000 | Mean Rewards: 283.00 |\n",
      "| Steps: 535000 | Mean Rewards: 271.04 |\n",
      "| Steps: 536000 | Mean Rewards: 278.44 |\n",
      "| Steps: 537000 | Mean Rewards: 308.64 |\n",
      "| Steps: 538000 | Mean Rewards: 296.68 |\n",
      "| Steps: 539000 | Mean Rewards: 269.16 |\n",
      "| Steps: 540000 | Mean Rewards: 254.12 |\n",
      "| Steps: 541000 | Mean Rewards: 225.00 |\n",
      "| Steps: 542000 | Mean Rewards: 215.68 |\n",
      "| Steps: 543000 | Mean Rewards: 202.36 |\n",
      "| Steps: 544000 | Mean Rewards: 205.40 |\n",
      "| Steps: 545000 | Mean Rewards: 192.56 |\n",
      "| Steps: 546000 | Mean Rewards: 223.44 |\n",
      "| Steps: 547000 | Mean Rewards: 266.00 |\n",
      "| Steps: 548000 | Mean Rewards: 292.44 |\n",
      "| Steps: 549000 | Mean Rewards: 279.32 |\n",
      "| Steps: 550000 | Mean Rewards: 262.52 |\n",
      "| Steps: 551000 | Mean Rewards: 249.20 |\n",
      "| Steps: 552000 | Mean Rewards: 250.60 |\n",
      "| Steps: 553000 | Mean Rewards: 276.40 |\n",
      "| Steps: 554000 | Mean Rewards: 272.96 |\n",
      "| Steps: 555000 | Mean Rewards: 282.84 |\n",
      "| Steps: 556000 | Mean Rewards: 316.60 |\n",
      "| Steps: 557000 | Mean Rewards: 318.76 |\n",
      "| Steps: 558000 | Mean Rewards: 303.08 |\n",
      "| Steps: 559000 | Mean Rewards: 292.52 |\n",
      "| Steps: 560000 | Mean Rewards: 278.08 |\n",
      "| Steps: 561000 | Mean Rewards: 298.20 |\n",
      "| Steps: 562000 | Mean Rewards: 322.96 |\n",
      "| Steps: 563000 | Mean Rewards: 336.12 |\n",
      "| Steps: 564000 | Mean Rewards: 322.92 |\n",
      "| Steps: 565000 | Mean Rewards: 314.68 |\n",
      "| Steps: 566000 | Mean Rewards: 306.80 |\n",
      "| Steps: 567000 | Mean Rewards: 342.32 |\n",
      "| Steps: 568000 | Mean Rewards: 370.00 |\n",
      "| Steps: 569000 | Mean Rewards: 358.44 |\n",
      "| Steps: 570000 | Mean Rewards: 336.56 |\n",
      "| Steps: 571000 | Mean Rewards: 371.24 |\n",
      "| Steps: 572000 | Mean Rewards: 368.28 |\n",
      "| Steps: 573000 | Mean Rewards: 376.12 |\n",
      "| Steps: 574000 | Mean Rewards: 376.80 |\n",
      "| Steps: 575000 | Mean Rewards: 382.00 |\n",
      "| Steps: 576000 | Mean Rewards: 421.96 |\n",
      "| Steps: 577000 | Mean Rewards: 420.76 |\n",
      "| Steps: 578000 | Mean Rewards: 441.68 |\n",
      "| Steps: 579000 | Mean Rewards: 454.36 |\n",
      "| Steps: 580000 | Mean Rewards: 464.88 |\n",
      "| Steps: 581000 | Mean Rewards: 466.96 |\n",
      "| Steps: 582000 | Mean Rewards: 452.16 |\n",
      "| Steps: 583000 | Mean Rewards: 446.32 |\n",
      "| Steps: 584000 | Mean Rewards: 426.12 |\n",
      "| Steps: 585000 | Mean Rewards: 460.40 |\n",
      "| Steps: 586000 | Mean Rewards: 452.92 |\n",
      "| Steps: 587000 | Mean Rewards: 422.64 |\n",
      "| Steps: 588000 | Mean Rewards: 382.52 |\n",
      "| Steps: 589000 | Mean Rewards: 353.08 |\n",
      "| Steps: 590000 | Mean Rewards: 306.08 |\n",
      "| Steps: 591000 | Mean Rewards: 271.16 |\n",
      "| Steps: 592000 | Mean Rewards: 267.24 |\n",
      "| Steps: 593000 | Mean Rewards: 274.08 |\n",
      "| Steps: 594000 | Mean Rewards: 301.40 |\n",
      "| Steps: 595000 | Mean Rewards: 297.68 |\n",
      "| Steps: 596000 | Mean Rewards: 310.40 |\n",
      "| Steps: 597000 | Mean Rewards: 337.60 |\n",
      "| Steps: 598000 | Mean Rewards: 320.08 |\n",
      "| Steps: 599000 | Mean Rewards: 292.92 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Steps: 600000 | Mean Rewards: 267.28 |\n",
      "| Steps: 601000 | Mean Rewards: 307.32 |\n",
      "| Steps: 602000 | Mean Rewards: 345.68 |\n",
      "| Steps: 603000 | Mean Rewards: 353.92 |\n",
      "| Steps: 604000 | Mean Rewards: 319.68 |\n",
      "| Steps: 605000 | Mean Rewards: 295.80 |\n",
      "| Steps: 606000 | Mean Rewards: 314.76 |\n",
      "| Steps: 607000 | Mean Rewards: 329.72 |\n",
      "| Steps: 608000 | Mean Rewards: 390.28 |\n",
      "| Steps: 609000 | Mean Rewards: 422.48 |\n",
      "| Steps: 610000 | Mean Rewards: 406.08 |\n",
      "| Steps: 611000 | Mean Rewards: 376.20 |\n",
      "| Steps: 612000 | Mean Rewards: 385.68 |\n",
      "| Steps: 613000 | Mean Rewards: 373.92 |\n",
      "| Steps: 614000 | Mean Rewards: 410.20 |\n",
      "| Steps: 615000 | Mean Rewards: 437.12 |\n",
      "| Steps: 616000 | Mean Rewards: 416.48 |\n",
      "| Steps: 617000 | Mean Rewards: 437.56 |\n",
      "| Steps: 618000 | Mean Rewards: 450.60 |\n",
      "Reached reward threshold in 618400 steps\n"
     ]
    }
   ],
   "source": [
    "MAX_STEPS = 100_000\n",
    "N_UPDATE_STEPS =  100\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "N_TRIALS = 25\n",
    "REWARD_THRESHOLD = 475\n",
    "PRINT_EVERY = 10\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "_ = train_env.reset()\n",
    "\n",
    "for step in tqdm(range(MAX_STEPS)):\n",
    "        \n",
    "    policy_loss, value_loss = train(train_env, actor, critic, actor_optimizer, critic_optimizer, N_UPDATE_STEPS, DISCOUNT_FACTOR)\n",
    "    \n",
    "    episode_reward = evaluate(test_env, actor, critic)\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    \n",
    "    mean_trial_rewards = np.mean(episode_rewards[-N_TRIALS:])\n",
    "    \n",
    "    if step % PRINT_EVERY == 0:\n",
    "            \n",
    "        print(f'| Steps: {N_UPDATE_STEPS*step:6} | Mean Rewards: {mean_trial_rewards:6.2f} |')\n",
    "    \n",
    "    if mean_trial_rewards >= REWARD_THRESHOLD:\n",
    "        \n",
    "        print(f'Reached reward threshold in {N_UPDATE_STEPS*step} steps')\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
