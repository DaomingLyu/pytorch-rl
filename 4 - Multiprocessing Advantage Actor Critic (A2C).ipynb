{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/\n",
    "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f\n",
    "http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_5_actor_critic_pdf.pdf\n",
    "https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/\n",
    "https://danieltakeshi.github.io/2018/06/28/a2c-a3c/\n",
    "https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/\n",
    "https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/master/a2c_ppo_acktr/algo/a2c_acktr.py\n",
    "https://github.com/higgsfield/RL-Adventure-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from utils import SubprocVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "np.random.seed(SEED);\n",
    "torch.manual_seed(SEED);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ENVS = 3\n",
    "\n",
    "def make_env(env_name, seed):\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        env.seed(seed)\n",
    "        return env\n",
    "    return _thunk\n",
    "\n",
    "#all environments need different random seed!\n",
    "envs = [make_env('CartPole-v1', SEED+i) for i in range(N_ENVS)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(SEED)\n",
    "\n",
    "assert isinstance(envs.observation_space, gym.spaces.Box)\n",
    "assert isinstance(envs.action_space, gym.spaces.Discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout = 0.25):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc_1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = env.observation_space.shape[0]\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = env.action_space.n\n",
    "\n",
    "actor = MLP(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "critic = MLP(INPUT_DIM, HIDDEN_DIM, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc_1): Linear(in_features=4, out_features=128, bias=True)\n",
       "  (fc_2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.25)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "actor.apply(init_weights)\n",
    "critic.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr = LEARNING_RATE)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(envs, actor, critic, actor_optimizer, critic_optimizer, n_steps, discount_factor):\n",
    "    \n",
    "    log_prob_actions = torch.zeros(n_steps, len(envs))\n",
    "    values = torch.zeros(n_steps, len(envs))\n",
    "    rewards = torch.zeros(n_steps, len(envs))\n",
    "    dones = torch.zeros(n_steps, len(envs))\n",
    "    episode_reward = 0\n",
    "    \n",
    "    state = envs.get_state()\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "\n",
    "        state = torch.FloatTensor(state) #[n_envs, observation_space]\n",
    "\n",
    "        action_preds = actor(state) #[n_envs, action_space]\n",
    "        value_pred = critic(state).squeeze(-1) #[n_envs]\n",
    "            \n",
    "        action_probs = F.softmax(action_preds, dim = -1) #[n_envs, action_space]\n",
    "                            \n",
    "        dist = distributions.Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample() #[n_envs]\n",
    "                \n",
    "        log_prob_action = dist.log_prob(action) #[n_envs]\n",
    "                \n",
    "        #action now numpy array across all envs\n",
    "        state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "                \n",
    "        reward = torch.FloatTensor(reward) #[n_envs]\n",
    "\n",
    "        done = torch.FloatTensor(done) #[n_envs]\n",
    "        \n",
    "        log_prob_actions[step] = log_prob_action\n",
    "        values[step] = value_pred\n",
    "        rewards[step] = reward\n",
    "        dones[step] = done\n",
    "    \n",
    "    next_value = critic(torch.FloatTensor(state)).squeeze(-1)\n",
    "    returns = calculate_returns(rewards, next_value, dones, discount_factor)\n",
    "    advantages = calculate_advantages(returns, values)\n",
    "    \n",
    "    policy_loss, value_loss = update_policy(advantages, log_prob_actions, returns, values, actor_optimizer, critic_optimizer)\n",
    "\n",
    "    return policy_loss, value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, actor, critic, discount_factor):\n",
    "    \n",
    "    rewards = []\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        action_preds = actor(state)\n",
    "        \n",
    "        action_probs = F.softmax(action_preds, dim = -1)\n",
    "        \n",
    "        dist = distributions.Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample() \n",
    "        \n",
    "        state, reward, done, _ = env.step(action.item())\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(rewards, next_value, dones, discount_factor):\n",
    "    \n",
    "    R = next_value\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    \n",
    "    for i, (r, d) in enumerate(zip(reversed(rewards), reversed(dones))):\n",
    "        R = r + discount_factor * R * (1 - d) \n",
    "        returns[i] = R\n",
    "   \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advantages(returns, values, normalize = True):\n",
    "    \n",
    "    advantages = []\n",
    "    \n",
    "    for r, v in zip(reversed(returns), reversed(values)):\n",
    "        advantage = r - v\n",
    "        advantages.insert(0, advantage)\n",
    "        \n",
    "    advantages = torch.tensor(advantages)\n",
    "    \n",
    "    if normalize:\n",
    "        \n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "        \n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(advantages, log_prob_actions, returns, values, actor_optimizer, critic_optimizer):\n",
    "        \n",
    "    policy_loss = - (advantages * log_prob_actions).mean()\n",
    "    \n",
    "    value_loss = F.smooth_l1_loss(returns, values).mean()\n",
    "        \n",
    "    actor_optimizer.zero_grad()\n",
    "    critic_optimizer.zero_grad()\n",
    "    \n",
    "    policy_loss.backward()\n",
    "    value_loss.backward()\n",
    "    \n",
    "    actor_optimizer.step()\n",
    "    critic_optimizer.step()\n",
    "    \n",
    "    return policy_loss.item(), value_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641026850d2045e88c6635471c941a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9996, 0.9464, 0.9872],\n",
      "        [1.9896, 1.9369, 1.9773],\n",
      "        [2.9697, 2.9176, 2.9575],\n",
      "        [3.9400, 3.8884, 3.9280],\n",
      "        [4.9006, 4.8495, 4.8887]], grad_fn=<CopySlices>)\n",
      "tensor([[ 0.0026, -0.0064,  0.0029],\n",
      "        [-0.0378, -0.0356, -0.0300],\n",
      "        [ 0.0037, -0.0749,  0.0056],\n",
      "        [-0.0011, -0.0497, -0.0647],\n",
      "        [-0.0765, -0.0007,  0.0003]], grad_fn=<CopySlices>)\n",
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n",
      "tensor([[4.9771, 4.8502, 4.8884],\n",
      "        [3.9411, 3.9381, 3.9926],\n",
      "        [2.9660, 2.9925, 2.9519],\n",
      "        [2.0274, 1.9725, 2.0073],\n",
      "        [0.9970, 0.9528, 0.9842]], grad_fn=<CopySlices>)\n",
      "tensor([[0.9970, 0.9528, 0.9842],\n",
      "        [2.0274, 1.9725, 2.0073],\n",
      "        [2.9660, 2.9925, 2.9519],\n",
      "        [3.9411, 3.9381, 3.9926],\n",
      "        [4.9771, 4.8502, 4.8884]], grad_fn=<SubBackward0>)\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-fe60e403800a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_STEPS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_UPDATE_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_UPDATE_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDISCOUNT_FACTOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mepisode_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDISCOUNT_FACTOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-2f667b449a48>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(envs, actor, critic, actor_optimizer, critic_optimizer, n_steps, discount_factor)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mnext_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mreturns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_returns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0madvantages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_advantages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madvantages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-3e6a33198765>\u001b[0m in \u001b[0;36mcalculate_advantages\u001b[0;34m(returns, values, normalize)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0madvantages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MAX_STEPS = 100_000\n",
    "N_UPDATE_STEPS =  5\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "N_TRIALS = 25\n",
    "REWARD_THRESHOLD = 475\n",
    "PRINT_EVERY = 10\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "_ = envs.reset()\n",
    "\n",
    "for step in tqdm(range(1, MAX_STEPS+1, N_UPDATE_STEPS)):\n",
    "        \n",
    "    policy_loss, value_loss = train(envs, actor, critic, actor_optimizer, critic_optimizer, N_UPDATE_STEPS, DISCOUNT_FACTOR)\n",
    "    \n",
    "    episode_reward = evaluate(env, actor, critic, DISCOUNT_FACTOR)\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    \n",
    "    mean_trial_rewards = np.mean(episode_rewards[-N_TRIALS:])\n",
    "    \n",
    "    if step % (1+(N_UPDATE_STEPS*PRINT_EVERY)) == 0:\n",
    "            \n",
    "        print(f'| Steps: {step:6} | Mean Rewards: {mean_trial_rewards:6.2f} |')\n",
    "    \n",
    "    if mean_trial_rewards >= REWARD_THRESHOLD:\n",
    "        \n",
    "        print(f'Reached reward threshold in {episode} episodes')\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode', fontsize=20)\n",
    "plt.ylabel('Reward', fontsize=20)\n",
    "plt.hlines(REWARD_THRESHOLD, 0, len(episode_rewards), color='r')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['a','b','c','d','e']\n",
    "\n",
    "for i, n in enumerate(reversed(x)):\n",
    "    print(i, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = torch.randn(3,1)\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
