{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/\n",
    "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f\n",
    "http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_5_actor_critic_pdf.pdf\n",
    "https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/\n",
    "https://danieltakeshi.github.io/2018/06/28/a2c-a3c/\n",
    "https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/\n",
    "https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/master/a2c_ppo_acktr/algo/a2c_acktr.py\n",
    "https://github.com/higgsfield/RL-Adventure-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from utils import SubprocVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "np.random.seed(SEED);\n",
    "torch.manual_seed(SEED);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ENVS = 16\n",
    "\n",
    "def make_env(env_name, seed):\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        env.seed(seed)\n",
    "        return env\n",
    "    return _thunk\n",
    "\n",
    "#all environments need different random seed!\n",
    "envs = [make_env('CartPole-v1', SEED+i) for i in range(N_ENVS)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(SEED)\n",
    "\n",
    "assert isinstance(envs.observation_space, gym.spaces.Box)\n",
    "assert isinstance(envs.action_space, gym.spaces.Discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout = 0.25):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc_1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = env.observation_space.shape[0]\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = env.action_space.n\n",
    "\n",
    "actor = MLP(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "critic = MLP(INPUT_DIM, HIDDEN_DIM, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc_1): Linear(in_features=4, out_features=128, bias=True)\n",
       "  (fc_2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.25)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "actor.apply(init_weights)\n",
    "critic.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr = LEARNING_RATE)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(envs, actor, critic, actor_optimizer, critic_optimizer, n_steps, discount_factor):\n",
    "    \n",
    "    log_prob_actions = torch.zeros(n_steps, len(envs))\n",
    "    entropies = torch.zeros(n_steps, len(envs))\n",
    "    values = torch.zeros(n_steps, len(envs))\n",
    "    rewards = torch.zeros(n_steps, len(envs))\n",
    "    masks = torch.zeros(n_steps, len(envs))\n",
    "    episode_reward = 0\n",
    "    \n",
    "    state = envs.get_state()\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "\n",
    "        state = torch.FloatTensor(state) #[n_envs, observation_space]\n",
    "\n",
    "        action_preds = actor(state) #[n_envs, action_space]\n",
    "        value_pred = critic(state).squeeze(-1) #[n_envs]\n",
    "            \n",
    "        action_probs = F.softmax(action_preds, dim = -1) #[n_envs, action_space]\n",
    "                            \n",
    "        dist = distributions.Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample() #[n_envs]\n",
    "                \n",
    "        log_prob_action = dist.log_prob(action) #[n_envs]\n",
    "                \n",
    "        entropy = dist.entropy() #[n_envs]\n",
    "            \n",
    "        #action now numpy array across all envs\n",
    "        state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "                \n",
    "        reward = torch.FloatTensor(reward) #[n_envs]\n",
    "\n",
    "        mask = torch.FloatTensor(1 - done) #[n_envs]\n",
    "        \n",
    "        log_prob_actions[step] = log_prob_action\n",
    "        entropies[step] = entropy\n",
    "        values[step] = value_pred\n",
    "        rewards[step] = reward\n",
    "        masks[step] = mask\n",
    "    \n",
    "    next_value = critic(torch.FloatTensor(state)).squeeze(-1)\n",
    "    \n",
    "    returns = calculate_returns(rewards, next_value, masks, discount_factor)\n",
    "    advantages = calculate_advantages(returns, values)\n",
    "    \n",
    "    policy_loss, value_loss = update_policy(advantages, log_prob_actions, returns, values, entropies, actor_optimizer, critic_optimizer)\n",
    "\n",
    "    return policy_loss, value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, actor, critic):\n",
    "    \n",
    "    rewards = []\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        action_preds = actor(state)\n",
    "        \n",
    "        action_probs = F.softmax(action_preds, dim = -1)\n",
    "        \n",
    "        dist = distributions.Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample() \n",
    "        \n",
    "        state, reward, done, _ = env.step(action.item())\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(rewards, next_value, masks, discount_factor, normalize = True):\n",
    "    \n",
    "    R = next_value\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    \n",
    "    for i, (r, m) in enumerate(zip(reversed(rewards), reversed(masks))):\n",
    "        R = r + discount_factor * R * m\n",
    "        returns[i] = R\n",
    "   \n",
    "    if normalize:\n",
    "        returns = (returns - returns.mean()) / returns.std()\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advantages(returns, values, normalize = False):\n",
    "    \n",
    "    advantages = returns - values\n",
    "    \n",
    "    if normalize:\n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "        \n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(advantages, log_prob_actions, returns, values, entropies, actor_optimizer, critic_optimizer):\n",
    "        \n",
    "    advantages = advantages.detach()\n",
    "    returns = returns.detach()\n",
    "    \n",
    "    policy_loss = - (advantages * log_prob_actions).mean() - 0.001 * entropies.mean()\n",
    "    \n",
    "    value_loss = F.smooth_l1_loss(returns, values).mean()\n",
    "        \n",
    "    actor_optimizer.zero_grad()\n",
    "    critic_optimizer.zero_grad()\n",
    "    \n",
    "    policy_loss.backward()\n",
    "    value_loss.backward()\n",
    "    \n",
    "    actor_optimizer.step()\n",
    "    critic_optimizer.step()\n",
    "    \n",
    "    return policy_loss.item(), value_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5de539446e40b8b03363c8fc300f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Steps:     51 | Mean Rewards:  23.82 |\n",
      "| Steps:    306 | Mean Rewards:  30.24 |\n",
      "| Steps:    561 | Mean Rewards:  28.04 |\n",
      "| Steps:    816 | Mean Rewards:  26.64 |\n",
      "| Steps:   1071 | Mean Rewards:  28.24 |\n",
      "| Steps:   1326 | Mean Rewards:  27.72 |\n",
      "| Steps:   1581 | Mean Rewards:  25.96 |\n",
      "| Steps:   1836 | Mean Rewards:  30.88 |\n",
      "| Steps:   2091 | Mean Rewards:  34.64 |\n",
      "| Steps:   2346 | Mean Rewards:  25.52 |\n",
      "| Steps:   2601 | Mean Rewards:  28.24 |\n",
      "| Steps:   2856 | Mean Rewards:  26.84 |\n",
      "| Steps:   3111 | Mean Rewards:  27.24 |\n",
      "| Steps:   3366 | Mean Rewards:  33.76 |\n",
      "| Steps:   3621 | Mean Rewards:  28.56 |\n",
      "| Steps:   3876 | Mean Rewards:  28.12 |\n",
      "| Steps:   4131 | Mean Rewards:  24.28 |\n",
      "| Steps:   4386 | Mean Rewards:  28.28 |\n",
      "| Steps:   4641 | Mean Rewards:  22.28 |\n",
      "| Steps:   4896 | Mean Rewards:  25.76 |\n",
      "| Steps:   5151 | Mean Rewards:  27.00 |\n",
      "| Steps:   5406 | Mean Rewards:  26.04 |\n",
      "| Steps:   5661 | Mean Rewards:  23.08 |\n",
      "| Steps:   5916 | Mean Rewards:  18.52 |\n",
      "| Steps:   6171 | Mean Rewards:  24.72 |\n",
      "| Steps:   6426 | Mean Rewards:  24.40 |\n",
      "| Steps:   6681 | Mean Rewards:  30.00 |\n",
      "| Steps:   6936 | Mean Rewards:  20.88 |\n",
      "| Steps:   7191 | Mean Rewards:  24.92 |\n",
      "| Steps:   7446 | Mean Rewards:  20.68 |\n",
      "| Steps:   7701 | Mean Rewards:  30.12 |\n",
      "| Steps:   7956 | Mean Rewards:  29.84 |\n",
      "| Steps:   8211 | Mean Rewards:  17.96 |\n",
      "| Steps:   8466 | Mean Rewards:  23.60 |\n",
      "| Steps:   8721 | Mean Rewards:  23.00 |\n",
      "| Steps:   8976 | Mean Rewards:  27.36 |\n",
      "| Steps:   9231 | Mean Rewards:  26.80 |\n",
      "| Steps:   9486 | Mean Rewards:  30.44 |\n",
      "| Steps:   9741 | Mean Rewards:  25.40 |\n",
      "| Steps:   9996 | Mean Rewards:  25.12 |\n",
      "| Steps:  10251 | Mean Rewards:  26.08 |\n",
      "| Steps:  10506 | Mean Rewards:  24.84 |\n",
      "| Steps:  10761 | Mean Rewards:  23.88 |\n",
      "| Steps:  11016 | Mean Rewards:  28.56 |\n",
      "| Steps:  11271 | Mean Rewards:  28.84 |\n",
      "| Steps:  11526 | Mean Rewards:  26.24 |\n",
      "| Steps:  11781 | Mean Rewards:  23.52 |\n",
      "| Steps:  12036 | Mean Rewards:  22.12 |\n",
      "| Steps:  12291 | Mean Rewards:  23.56 |\n",
      "| Steps:  12546 | Mean Rewards:  25.52 |\n",
      "| Steps:  12801 | Mean Rewards:  19.72 |\n",
      "| Steps:  13056 | Mean Rewards:  24.36 |\n",
      "| Steps:  13311 | Mean Rewards:  32.12 |\n",
      "| Steps:  13566 | Mean Rewards:  27.56 |\n",
      "| Steps:  13821 | Mean Rewards:  23.48 |\n",
      "| Steps:  14076 | Mean Rewards:  22.08 |\n",
      "| Steps:  14331 | Mean Rewards:  35.96 |\n",
      "| Steps:  14586 | Mean Rewards:  27.48 |\n",
      "| Steps:  14841 | Mean Rewards:  38.68 |\n",
      "| Steps:  15096 | Mean Rewards:  31.92 |\n",
      "| Steps:  15351 | Mean Rewards:  32.76 |\n",
      "| Steps:  15606 | Mean Rewards:  31.28 |\n",
      "| Steps:  15861 | Mean Rewards:  30.80 |\n",
      "| Steps:  16116 | Mean Rewards:  31.00 |\n",
      "| Steps:  16371 | Mean Rewards:  28.68 |\n",
      "| Steps:  16626 | Mean Rewards:  24.76 |\n",
      "| Steps:  16881 | Mean Rewards:  25.44 |\n",
      "| Steps:  17136 | Mean Rewards:  28.40 |\n",
      "| Steps:  17391 | Mean Rewards:  31.80 |\n",
      "| Steps:  17646 | Mean Rewards:  26.44 |\n",
      "| Steps:  17901 | Mean Rewards:  26.20 |\n",
      "| Steps:  18156 | Mean Rewards:  29.84 |\n",
      "| Steps:  18411 | Mean Rewards:  28.32 |\n",
      "| Steps:  18666 | Mean Rewards:  25.92 |\n",
      "| Steps:  18921 | Mean Rewards:  35.96 |\n",
      "| Steps:  19176 | Mean Rewards:  33.60 |\n",
      "| Steps:  19431 | Mean Rewards:  32.80 |\n",
      "| Steps:  19686 | Mean Rewards:  38.08 |\n",
      "| Steps:  19941 | Mean Rewards:  30.52 |\n",
      "| Steps:  20196 | Mean Rewards:  37.12 |\n",
      "| Steps:  20451 | Mean Rewards:  40.00 |\n",
      "| Steps:  20706 | Mean Rewards:  31.76 |\n",
      "| Steps:  20961 | Mean Rewards:  34.04 |\n",
      "| Steps:  21216 | Mean Rewards:  39.00 |\n",
      "| Steps:  21471 | Mean Rewards:  41.84 |\n",
      "| Steps:  21726 | Mean Rewards:  32.08 |\n",
      "| Steps:  21981 | Mean Rewards:  37.08 |\n",
      "| Steps:  22236 | Mean Rewards:  38.12 |\n",
      "| Steps:  22491 | Mean Rewards:  47.60 |\n",
      "| Steps:  22746 | Mean Rewards:  46.56 |\n",
      "| Steps:  23001 | Mean Rewards:  49.20 |\n",
      "| Steps:  23256 | Mean Rewards:  45.56 |\n",
      "| Steps:  23511 | Mean Rewards:  42.04 |\n",
      "| Steps:  23766 | Mean Rewards:  45.48 |\n",
      "| Steps:  24021 | Mean Rewards:  54.48 |\n",
      "| Steps:  24276 | Mean Rewards:  41.60 |\n",
      "| Steps:  24531 | Mean Rewards:  51.96 |\n",
      "| Steps:  24786 | Mean Rewards:  63.48 |\n",
      "| Steps:  25041 | Mean Rewards:  64.04 |\n",
      "| Steps:  25296 | Mean Rewards:  62.84 |\n",
      "| Steps:  25551 | Mean Rewards:  55.88 |\n",
      "| Steps:  25806 | Mean Rewards:  39.48 |\n",
      "| Steps:  26061 | Mean Rewards:  39.72 |\n",
      "| Steps:  26316 | Mean Rewards:  51.44 |\n",
      "| Steps:  26571 | Mean Rewards:  44.76 |\n",
      "| Steps:  26826 | Mean Rewards:  67.20 |\n",
      "| Steps:  27081 | Mean Rewards:  60.36 |\n",
      "| Steps:  27336 | Mean Rewards:  52.92 |\n",
      "| Steps:  27591 | Mean Rewards:  55.76 |\n",
      "| Steps:  27846 | Mean Rewards:  43.72 |\n",
      "| Steps:  28101 | Mean Rewards:  55.64 |\n",
      "| Steps:  28356 | Mean Rewards:  52.96 |\n",
      "| Steps:  28611 | Mean Rewards:  55.36 |\n",
      "| Steps:  28866 | Mean Rewards:  80.36 |\n",
      "| Steps:  29121 | Mean Rewards:  52.00 |\n",
      "| Steps:  29376 | Mean Rewards:  61.28 |\n",
      "| Steps:  29631 | Mean Rewards:  59.44 |\n",
      "| Steps:  29886 | Mean Rewards:  51.28 |\n",
      "| Steps:  30141 | Mean Rewards:  54.88 |\n",
      "| Steps:  30396 | Mean Rewards:  60.36 |\n",
      "| Steps:  30651 | Mean Rewards:  54.92 |\n",
      "| Steps:  30906 | Mean Rewards:  76.36 |\n",
      "| Steps:  31161 | Mean Rewards:  63.16 |\n",
      "| Steps:  31416 | Mean Rewards:  57.96 |\n",
      "| Steps:  31671 | Mean Rewards:  81.60 |\n",
      "| Steps:  31926 | Mean Rewards:  63.24 |\n",
      "| Steps:  32181 | Mean Rewards:  67.68 |\n",
      "| Steps:  32436 | Mean Rewards:  68.60 |\n",
      "| Steps:  32691 | Mean Rewards:  64.08 |\n",
      "| Steps:  32946 | Mean Rewards:  85.04 |\n",
      "| Steps:  33201 | Mean Rewards:  72.84 |\n",
      "| Steps:  33456 | Mean Rewards:  74.52 |\n",
      "| Steps:  33711 | Mean Rewards:  67.84 |\n",
      "| Steps:  33966 | Mean Rewards:  69.48 |\n",
      "| Steps:  34221 | Mean Rewards:  70.60 |\n",
      "| Steps:  34476 | Mean Rewards:  75.76 |\n",
      "| Steps:  34731 | Mean Rewards:  62.60 |\n",
      "| Steps:  34986 | Mean Rewards:  87.16 |\n",
      "| Steps:  35241 | Mean Rewards:  80.12 |\n",
      "| Steps:  35496 | Mean Rewards:  85.72 |\n",
      "| Steps:  35751 | Mean Rewards: 101.04 |\n",
      "| Steps:  36006 | Mean Rewards:  67.40 |\n",
      "| Steps:  36261 | Mean Rewards:  79.24 |\n",
      "| Steps:  36516 | Mean Rewards:  89.40 |\n",
      "| Steps:  36771 | Mean Rewards:  82.40 |\n",
      "| Steps:  37026 | Mean Rewards:  77.88 |\n",
      "| Steps:  37281 | Mean Rewards:  87.04 |\n",
      "| Steps:  37536 | Mean Rewards:  83.44 |\n",
      "| Steps:  37791 | Mean Rewards:  68.36 |\n",
      "| Steps:  38046 | Mean Rewards:  85.36 |\n",
      "| Steps:  38301 | Mean Rewards:  90.52 |\n",
      "| Steps:  38556 | Mean Rewards:  98.56 |\n",
      "| Steps:  38811 | Mean Rewards:  86.92 |\n",
      "| Steps:  39066 | Mean Rewards:  81.48 |\n",
      "| Steps:  39321 | Mean Rewards:  95.52 |\n",
      "| Steps:  39576 | Mean Rewards:  98.80 |\n",
      "| Steps:  39831 | Mean Rewards: 109.08 |\n",
      "| Steps:  40086 | Mean Rewards: 117.08 |\n",
      "| Steps:  40341 | Mean Rewards:  88.84 |\n",
      "| Steps:  40596 | Mean Rewards: 104.44 |\n",
      "| Steps:  40851 | Mean Rewards: 105.20 |\n",
      "| Steps:  41106 | Mean Rewards: 103.24 |\n",
      "| Steps:  41361 | Mean Rewards:  83.40 |\n",
      "| Steps:  41616 | Mean Rewards:  93.56 |\n",
      "| Steps:  41871 | Mean Rewards: 109.72 |\n",
      "| Steps:  42126 | Mean Rewards: 114.20 |\n",
      "| Steps:  42381 | Mean Rewards:  96.00 |\n",
      "| Steps:  42636 | Mean Rewards: 112.56 |\n",
      "| Steps:  42891 | Mean Rewards: 102.80 |\n",
      "| Steps:  43146 | Mean Rewards:  98.00 |\n",
      "| Steps:  43401 | Mean Rewards:  70.12 |\n",
      "| Steps:  43656 | Mean Rewards:  82.00 |\n",
      "| Steps:  43911 | Mean Rewards:  77.20 |\n",
      "| Steps:  44166 | Mean Rewards:  86.40 |\n",
      "| Steps:  44421 | Mean Rewards:  89.56 |\n",
      "| Steps:  44676 | Mean Rewards:  88.44 |\n",
      "| Steps:  44931 | Mean Rewards:  82.32 |\n",
      "| Steps:  45186 | Mean Rewards: 101.56 |\n",
      "| Steps:  45441 | Mean Rewards:  95.08 |\n",
      "| Steps:  45696 | Mean Rewards: 102.24 |\n",
      "| Steps:  45951 | Mean Rewards: 106.40 |\n",
      "| Steps:  46206 | Mean Rewards: 143.28 |\n",
      "| Steps:  46461 | Mean Rewards: 108.72 |\n",
      "| Steps:  46716 | Mean Rewards: 107.68 |\n",
      "| Steps:  46971 | Mean Rewards: 134.00 |\n",
      "| Steps:  47226 | Mean Rewards: 132.32 |\n",
      "| Steps:  47481 | Mean Rewards: 109.76 |\n",
      "| Steps:  47736 | Mean Rewards: 106.68 |\n",
      "| Steps:  47991 | Mean Rewards:  83.44 |\n",
      "| Steps:  48246 | Mean Rewards: 139.76 |\n",
      "| Steps:  48501 | Mean Rewards: 117.04 |\n",
      "| Steps:  48756 | Mean Rewards: 128.80 |\n",
      "| Steps:  49011 | Mean Rewards: 130.68 |\n",
      "| Steps:  49266 | Mean Rewards: 132.16 |\n",
      "| Steps:  49521 | Mean Rewards: 111.36 |\n",
      "| Steps:  49776 | Mean Rewards: 104.36 |\n",
      "| Steps:  50031 | Mean Rewards: 116.84 |\n",
      "| Steps:  50286 | Mean Rewards: 121.28 |\n",
      "| Steps:  50541 | Mean Rewards: 113.76 |\n",
      "| Steps:  50796 | Mean Rewards: 118.92 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Steps:  51051 | Mean Rewards: 121.12 |\n",
      "| Steps:  51306 | Mean Rewards:  94.96 |\n",
      "| Steps:  51561 | Mean Rewards:  98.24 |\n",
      "| Steps:  51816 | Mean Rewards: 103.76 |\n",
      "| Steps:  52071 | Mean Rewards:  85.56 |\n",
      "| Steps:  52326 | Mean Rewards:  77.28 |\n",
      "| Steps:  52581 | Mean Rewards:  89.72 |\n",
      "| Steps:  52836 | Mean Rewards: 118.24 |\n",
      "| Steps:  53091 | Mean Rewards: 106.20 |\n",
      "| Steps:  53346 | Mean Rewards: 131.72 |\n",
      "| Steps:  53601 | Mean Rewards: 121.96 |\n",
      "| Steps:  53856 | Mean Rewards: 104.48 |\n",
      "| Steps:  54111 | Mean Rewards: 149.56 |\n",
      "| Steps:  54366 | Mean Rewards: 138.04 |\n",
      "| Steps:  54621 | Mean Rewards: 152.72 |\n",
      "| Steps:  54876 | Mean Rewards: 167.84 |\n",
      "| Steps:  55131 | Mean Rewards: 198.20 |\n",
      "| Steps:  55386 | Mean Rewards: 190.56 |\n",
      "| Steps:  55641 | Mean Rewards: 168.96 |\n",
      "| Steps:  55896 | Mean Rewards: 172.44 |\n",
      "| Steps:  56151 | Mean Rewards: 199.80 |\n",
      "| Steps:  56406 | Mean Rewards: 186.56 |\n",
      "| Steps:  56661 | Mean Rewards: 185.68 |\n",
      "| Steps:  56916 | Mean Rewards: 156.64 |\n",
      "| Steps:  57171 | Mean Rewards: 205.12 |\n",
      "| Steps:  57426 | Mean Rewards: 193.60 |\n",
      "| Steps:  57681 | Mean Rewards: 176.56 |\n",
      "| Steps:  57936 | Mean Rewards: 159.00 |\n",
      "| Steps:  58191 | Mean Rewards: 165.60 |\n",
      "| Steps:  58446 | Mean Rewards: 125.40 |\n",
      "| Steps:  58701 | Mean Rewards: 207.48 |\n",
      "| Steps:  58956 | Mean Rewards: 195.08 |\n",
      "| Steps:  59211 | Mean Rewards: 229.16 |\n",
      "| Steps:  59466 | Mean Rewards: 187.36 |\n",
      "| Steps:  59721 | Mean Rewards: 169.44 |\n",
      "| Steps:  59976 | Mean Rewards: 177.44 |\n",
      "| Steps:  60231 | Mean Rewards: 151.84 |\n",
      "| Steps:  60486 | Mean Rewards: 155.56 |\n",
      "| Steps:  60741 | Mean Rewards: 152.20 |\n",
      "| Steps:  60996 | Mean Rewards: 151.92 |\n",
      "| Steps:  61251 | Mean Rewards: 177.44 |\n",
      "| Steps:  61506 | Mean Rewards: 220.92 |\n",
      "| Steps:  61761 | Mean Rewards: 208.24 |\n",
      "| Steps:  62016 | Mean Rewards: 210.40 |\n",
      "| Steps:  62271 | Mean Rewards: 194.00 |\n",
      "| Steps:  62526 | Mean Rewards: 142.20 |\n",
      "| Steps:  62781 | Mean Rewards: 203.76 |\n",
      "| Steps:  63036 | Mean Rewards: 182.92 |\n",
      "| Steps:  63291 | Mean Rewards: 188.76 |\n",
      "| Steps:  63546 | Mean Rewards: 162.24 |\n",
      "| Steps:  63801 | Mean Rewards: 167.84 |\n",
      "| Steps:  64056 | Mean Rewards: 166.88 |\n",
      "| Steps:  64311 | Mean Rewards: 157.08 |\n",
      "| Steps:  64566 | Mean Rewards: 154.68 |\n",
      "| Steps:  64821 | Mean Rewards: 173.32 |\n",
      "| Steps:  65076 | Mean Rewards: 143.88 |\n",
      "| Steps:  65331 | Mean Rewards: 158.72 |\n",
      "| Steps:  65586 | Mean Rewards: 164.80 |\n",
      "| Steps:  65841 | Mean Rewards: 148.32 |\n",
      "| Steps:  66096 | Mean Rewards: 172.96 |\n",
      "| Steps:  66351 | Mean Rewards: 151.64 |\n",
      "| Steps:  66606 | Mean Rewards: 140.96 |\n",
      "| Steps:  66861 | Mean Rewards: 137.32 |\n",
      "| Steps:  67116 | Mean Rewards: 148.24 |\n",
      "| Steps:  67371 | Mean Rewards: 135.76 |\n",
      "| Steps:  67626 | Mean Rewards: 139.84 |\n",
      "| Steps:  67881 | Mean Rewards: 142.20 |\n",
      "| Steps:  68136 | Mean Rewards: 144.40 |\n",
      "| Steps:  68391 | Mean Rewards: 121.32 |\n",
      "| Steps:  68646 | Mean Rewards: 112.52 |\n",
      "| Steps:  68901 | Mean Rewards: 128.44 |\n",
      "| Steps:  69156 | Mean Rewards: 135.08 |\n",
      "| Steps:  69411 | Mean Rewards: 151.84 |\n",
      "| Steps:  69666 | Mean Rewards: 155.56 |\n",
      "| Steps:  69921 | Mean Rewards: 149.20 |\n",
      "| Steps:  70176 | Mean Rewards: 174.28 |\n",
      "| Steps:  70431 | Mean Rewards: 155.72 |\n",
      "| Steps:  70686 | Mean Rewards: 153.28 |\n",
      "| Steps:  70941 | Mean Rewards: 147.84 |\n",
      "| Steps:  71196 | Mean Rewards: 185.36 |\n",
      "| Steps:  71451 | Mean Rewards: 217.64 |\n",
      "| Steps:  71706 | Mean Rewards: 145.16 |\n",
      "| Steps:  71961 | Mean Rewards: 218.16 |\n",
      "| Steps:  72216 | Mean Rewards: 209.84 |\n",
      "| Steps:  72471 | Mean Rewards: 253.04 |\n",
      "| Steps:  72726 | Mean Rewards: 196.96 |\n",
      "| Steps:  72981 | Mean Rewards: 196.96 |\n",
      "| Steps:  73236 | Mean Rewards: 184.04 |\n",
      "| Steps:  73491 | Mean Rewards: 255.20 |\n",
      "| Steps:  73746 | Mean Rewards: 267.48 |\n",
      "| Steps:  74001 | Mean Rewards: 280.00 |\n",
      "| Steps:  74256 | Mean Rewards: 250.68 |\n",
      "| Steps:  74511 | Mean Rewards: 222.80 |\n",
      "| Steps:  74766 | Mean Rewards: 260.00 |\n",
      "| Steps:  75021 | Mean Rewards: 242.68 |\n",
      "| Steps:  75276 | Mean Rewards: 291.72 |\n",
      "| Steps:  75531 | Mean Rewards: 280.44 |\n",
      "| Steps:  75786 | Mean Rewards: 279.20 |\n",
      "| Steps:  76041 | Mean Rewards: 254.16 |\n",
      "| Steps:  76296 | Mean Rewards: 273.96 |\n",
      "| Steps:  76551 | Mean Rewards: 242.76 |\n",
      "| Steps:  76806 | Mean Rewards: 254.20 |\n",
      "| Steps:  77061 | Mean Rewards: 190.16 |\n",
      "| Steps:  77316 | Mean Rewards: 253.44 |\n",
      "| Steps:  77571 | Mean Rewards: 239.56 |\n",
      "| Steps:  77826 | Mean Rewards: 206.76 |\n",
      "| Steps:  78081 | Mean Rewards: 241.08 |\n",
      "| Steps:  78336 | Mean Rewards: 228.12 |\n",
      "| Steps:  78591 | Mean Rewards: 201.00 |\n",
      "| Steps:  78846 | Mean Rewards: 197.48 |\n",
      "| Steps:  79101 | Mean Rewards: 207.36 |\n",
      "| Steps:  79356 | Mean Rewards: 217.56 |\n",
      "| Steps:  79611 | Mean Rewards: 166.80 |\n",
      "| Steps:  79866 | Mean Rewards: 144.04 |\n",
      "| Steps:  80121 | Mean Rewards: 145.92 |\n",
      "| Steps:  80376 | Mean Rewards: 114.88 |\n",
      "| Steps:  80631 | Mean Rewards: 118.88 |\n",
      "| Steps:  80886 | Mean Rewards: 136.44 |\n",
      "| Steps:  81141 | Mean Rewards: 125.60 |\n",
      "| Steps:  81396 | Mean Rewards: 123.28 |\n",
      "| Steps:  81651 | Mean Rewards: 130.80 |\n",
      "| Steps:  81906 | Mean Rewards: 151.84 |\n",
      "| Steps:  82161 | Mean Rewards: 159.80 |\n",
      "| Steps:  82416 | Mean Rewards: 150.64 |\n",
      "| Steps:  82671 | Mean Rewards: 190.08 |\n",
      "| Steps:  82926 | Mean Rewards: 152.68 |\n",
      "| Steps:  83181 | Mean Rewards: 167.92 |\n",
      "| Steps:  83436 | Mean Rewards: 179.48 |\n",
      "| Steps:  83691 | Mean Rewards: 160.48 |\n",
      "| Steps:  83946 | Mean Rewards: 183.28 |\n",
      "| Steps:  84201 | Mean Rewards: 136.76 |\n",
      "| Steps:  84456 | Mean Rewards: 123.68 |\n",
      "| Steps:  84711 | Mean Rewards: 156.72 |\n",
      "| Steps:  84966 | Mean Rewards: 168.00 |\n",
      "| Steps:  85221 | Mean Rewards: 124.48 |\n",
      "| Steps:  85476 | Mean Rewards: 145.12 |\n",
      "| Steps:  85731 | Mean Rewards: 140.88 |\n",
      "| Steps:  85986 | Mean Rewards: 153.16 |\n",
      "| Steps:  86241 | Mean Rewards: 136.12 |\n",
      "| Steps:  86496 | Mean Rewards: 160.72 |\n",
      "| Steps:  86751 | Mean Rewards: 143.56 |\n",
      "| Steps:  87006 | Mean Rewards: 131.84 |\n",
      "| Steps:  87261 | Mean Rewards: 150.68 |\n",
      "| Steps:  87516 | Mean Rewards: 145.28 |\n",
      "| Steps:  87771 | Mean Rewards: 136.60 |\n",
      "| Steps:  88026 | Mean Rewards: 142.88 |\n",
      "| Steps:  88281 | Mean Rewards: 135.20 |\n",
      "| Steps:  88536 | Mean Rewards: 135.52 |\n",
      "| Steps:  88791 | Mean Rewards: 160.88 |\n",
      "| Steps:  89046 | Mean Rewards: 153.12 |\n",
      "| Steps:  89301 | Mean Rewards: 148.64 |\n",
      "| Steps:  89556 | Mean Rewards: 144.68 |\n",
      "| Steps:  89811 | Mean Rewards: 147.52 |\n",
      "| Steps:  90066 | Mean Rewards: 142.24 |\n",
      "| Steps:  90321 | Mean Rewards: 150.96 |\n",
      "| Steps:  90576 | Mean Rewards: 154.08 |\n",
      "| Steps:  90831 | Mean Rewards: 157.64 |\n",
      "| Steps:  91086 | Mean Rewards: 192.40 |\n",
      "| Steps:  91341 | Mean Rewards: 183.96 |\n",
      "| Steps:  91596 | Mean Rewards: 158.76 |\n",
      "| Steps:  91851 | Mean Rewards: 169.32 |\n",
      "| Steps:  92106 | Mean Rewards: 142.76 |\n",
      "| Steps:  92361 | Mean Rewards: 155.96 |\n",
      "| Steps:  92616 | Mean Rewards: 135.24 |\n",
      "| Steps:  92871 | Mean Rewards: 142.00 |\n",
      "| Steps:  93126 | Mean Rewards: 149.72 |\n",
      "| Steps:  93381 | Mean Rewards: 136.36 |\n",
      "| Steps:  93636 | Mean Rewards: 132.56 |\n",
      "| Steps:  93891 | Mean Rewards: 165.16 |\n",
      "| Steps:  94146 | Mean Rewards: 167.16 |\n",
      "| Steps:  94401 | Mean Rewards: 182.92 |\n",
      "| Steps:  94656 | Mean Rewards: 201.92 |\n",
      "| Steps:  94911 | Mean Rewards: 140.32 |\n",
      "| Steps:  95166 | Mean Rewards: 188.36 |\n",
      "| Steps:  95421 | Mean Rewards: 185.72 |\n",
      "| Steps:  95676 | Mean Rewards: 162.92 |\n",
      "| Steps:  95931 | Mean Rewards: 206.24 |\n",
      "| Steps:  96186 | Mean Rewards: 177.64 |\n",
      "| Steps:  96441 | Mean Rewards: 162.32 |\n",
      "| Steps:  96696 | Mean Rewards: 144.12 |\n",
      "| Steps:  96951 | Mean Rewards: 141.28 |\n",
      "| Steps:  97206 | Mean Rewards: 182.44 |\n",
      "| Steps:  97461 | Mean Rewards: 122.36 |\n",
      "| Steps:  97716 | Mean Rewards: 134.84 |\n",
      "| Steps:  97971 | Mean Rewards: 132.08 |\n",
      "| Steps:  98226 | Mean Rewards:  93.08 |\n",
      "| Steps:  98481 | Mean Rewards: 108.60 |\n",
      "| Steps:  98736 | Mean Rewards: 110.08 |\n",
      "| Steps:  98991 | Mean Rewards: 102.60 |\n",
      "| Steps:  99246 | Mean Rewards: 122.44 |\n",
      "| Steps:  99501 | Mean Rewards: 112.12 |\n",
      "| Steps:  99756 | Mean Rewards: 109.76 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_STEPS = 100_000\n",
    "N_UPDATE_STEPS =  5\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "N_TRIALS = 25\n",
    "REWARD_THRESHOLD = 475\n",
    "PRINT_EVERY = 10\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "_ = envs.reset()\n",
    "\n",
    "for step in tqdm(range(1, MAX_STEPS+1, N_UPDATE_STEPS)):\n",
    "        \n",
    "    policy_loss, value_loss = train(envs, actor, critic, actor_optimizer, critic_optimizer, N_UPDATE_STEPS, DISCOUNT_FACTOR)\n",
    "    \n",
    "    episode_reward = evaluate(env, actor, critic)\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    \n",
    "    mean_trial_rewards = np.mean(episode_rewards[-N_TRIALS:])\n",
    "    \n",
    "    if step % (1+(N_UPDATE_STEPS*PRINT_EVERY)) == 0:\n",
    "            \n",
    "        print(f'| Steps: {step:6} | Mean Rewards: {mean_trial_rewards:6.2f} |')\n",
    "    \n",
    "    if mean_trial_rewards >= REWARD_THRESHOLD:\n",
    "        \n",
    "        print(f'Reached reward threshold in {episode} episodes')\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
